
Cantonese Conversation Tutor Application 
Product Requirements Document
Product Overview and Scope
The Cantonese Conversation Tutor is a desktop-oriented web application for Chinese-speaking learners
to practice conversational Cantonese. It emphasizes natural spoken dialogue with an AI tutor over
rote grammar or vocabulary drills. The scope is limited to web browsers on desktop (not optimized for
mobile) to facilitate a richer, microphone-enabled experience. Key characteristics of the product scope
include:
• 
• 
• 
• 
• 
Conversational Focus: Prioritizes realistic dialogue practice in Cantonese, helping users gain
f
luency through interaction, rather than grammar exercises or flashcards.
Speaking and Listening Emphasis: Users engage by speaking Cantonese and listening to AI
generated responses, simulating real conversation. Minimal text-based quizzes or explicit
grammar instructions are included (except as feedback/support).
Not a Formal Curriculum: The app is not designed as a full language course with levels on
reading/writing; it assumes users already understand Chinese characters (as native Mandarin
speakers, for example) and want to speak and listen in Cantonese. It therefore avoids
extensive explanation of basic grammar or writing systems.
In-Scope Features: Real-time speech recognition and transcription, interactive AI-driven
dialogue, spoken output via text-to-speech, and pronunciation feedback. The system can
optionally offer on-screen text (Chinese characters and phonetic hints) to aid understanding and
pronunciation.
Out-of-Scope: Mobile app interfaces, lengthy grammar tutorials, vocabulary drilling modules, or
writing exercises. The focus is speaking practice – features like multiple-choice quizzes or
spaced repetition flashcards are deliberately excluded to maintain emphasis on conversation.
User Stories
To clarify the product goals, here are representative user stories for our target users (Chinese speakers
learning Cantonese):
• 
• 
• 
• 
• 
As a Chinese-speaking learner of Cantonese, I want to practice speaking in everyday situations (like
greetings, ordering food, small talk) with an AI partner so that I can gain confidence in real
conversations.
As a learner, I want the system to understand my spoken Cantonese and respond naturally, so that the
experience feels like talking to a native speaker and I can improve my listening skills.
As a user, I want to receive feedback on my pronunciation and word usage after I speak, so I can
correct mistakes (e.g. tone errors or misused phrases) and improve over time.
As a busy adult learner, I want the practice sessions to be flexible and guided by AI, meaning if I
stumble or use the wrong word, the AI can gently correct me or help me rephrase, rather than
following a rigid script.
As a learner who can read Chinese characters, I want to see the Cantonese dialogue transcribed (in
Chinese characters, with optional phonetic romanization) so that I can visually connect what I hear to
written form and revisit the conversation later.
1
• 
As a product owner (or teacher), I want the system to track performance metrics like the accuracy of
user pronunciation and engagement (session length, frequency), so that we can measure the learner’s
progress and the app’s effectiveness.
These user stories drive the key features and design decisions for the application.
Key Features and Functional Requirements
Based on the scope and user stories, the Cantonese tutor will provide the following key features:
1. Interactive AI Conversation Mode
• 
• 
• 
• 
• 
AI-driven Dialogue: The core feature is a free-form conversation between the user and an AI 
Cantonese tutor persona. The AI will greet the user and carry on an open-ended conversation
on everyday topics. The tutor’s responses are generated by a Cantonese-capable LLM, ensuring
they are contextually relevant and entirely in Cantonese (written in Chinese characters for
display). This allows the user to practice natural language exchange.
Speech Input & Output: Users participate by speaking into their microphone. The app will use
real-time Automatic Speech Recognition (ASR) to transcribe the user’s Cantonese speech to text.
The AI then processes this text and formulates a Cantonese reply. The reply is delivered via text
to-speech (TTS) so the user hears the response spoken aloud, modeling correct pronunciation
and intonation.
Turn-Based Dialogue Flow: The conversation occurs in turns. The user either presses a “Hold to
Speak” button or uses voice activation to begin speaking. After the user finishes, the system
transcribes the input and the AI responds. The UI should display both the user’s recognized
speech (in Cantonese text) and the AI’s reply text, creating a chat transcript. Users can thus 
listen and read along.
Context Management: The system maintains context of the dialogue so that the AI’s responses
are coherent and refer back to previous user inputs. For example, if the user and AI have been
discussing food, the AI’s next responses stay on topic unless the user changes it. The backend
will accumulate a conversation history (recent turns) and include it when calling the LLM to
provide context. There may be a limit (e.g. last 10 turns) to maintain performance.
Scenario and Topic Selection: While free conversation is supported, the user can optionally
choose a scenario or topic to practice (e.g. “At the Restaurant”, “Meeting New People”,
“Traveling in Hong Kong”). The system will then prompt the AI tutor to stick to that scenario,
possibly starting with a relevant opening line. This guides learners who want a more focused
practice session. Scenario selection is done via the UI before conversation start (e.g. a dropdown
or menu of common situations).
2. Real-Time Speech Recognition (ASR)
• 
• 
Cantonese ASR Integration: The application will integrate a high-quality Automatic Speech
Recognition service that supports Cantonese. This component listens to the user’s spoken input
(audio) and produces a text transcription. The chosen ASR must handle the Cantonese
language (Yue Chinese), recognizing colloquial speech and various tones/accents. For instance,
Google Cloud Speech-to-Text supports Cantonese (Traditional Hong Kong, language code yue
Hant-HK)
1
2
, and Amazon Transcribe supports Cantonese (zh-HK, Yue) in both batch and
streaming modes . We will use the API in streaming or near-real-time mode to get quick
transcriptions.
Accuracy and Punctuation: The ASR engine should ideally provide accurate transcription with
proper handling of Cantonese-specific vocabulary. Automatic punctuation is a desirable feature
2
(Google’s API can provide automatic punctuation in transcripts
1
). The system will pass the raw
transcription into the LLM; however, we might also apply a secondary processing step (possibly
using the LLM) to clean up or punctuate the recognized text if needed. In testing, we’ll ensure
the recognition quality is sufficient for the LLM to understand the user – this is critical for a
smooth conversation.
• 
• 
Latency: Real-time feedback is important, so the ASR response should be fast (preferably under
1-2 seconds for typical utterances). If the chosen ASR API supports streaming, we can start
sending audio while the user is speaking and get partial results to reduce latency. The UI might
display interim transcription as the user speaks (optional).
Language Model Adaptation: The ASR service should be tuned or configured, if possible, with a
custom language model or vocabulary for Cantonese conversational content (e.g. common
learner phrases, proper nouns). For example, Azure’s Speech to Text allows model adaptation
and has full support for Cantonese (zh-HK) in its standard and custom models
3
. Initially, we
will rely on the vendor’s general Cantonese model, but the architecture will allow plugging in
improved models or custom phrase hints if necessary.
3. Conversational AI & Feedback Logic (LLM)
• 
• 
• 
• 
• 
• 
• 
LLM Integration: At the heart of the tutor’s intelligence is a Large Language Model (LLM) that
can understand Cantonese input and generate appropriate Cantonese responses. We plan to
use a state-of-the-art model (such as OpenAI’s GPT-4 via API) given its strong multilingual
capabilities. GPT-4 has demonstrated the ability to understand and generate Cantonese (Yue
Chinese)
4
, making it a suitable choice for driving the conversational agent. The model will be
prompted to adopt the role of a friendly Cantonese tutor: it will reply in Cantonese, keep the
conversation going, and gently assist the learner.
Prompting and System Behavior: Every time the user speaks, the backend will send a prompt
to the LLM including: a system instruction (defining the AI’s role and style, e.g. “You are a
Cantonese tutor conversing with the learner. Always respond in Cantonese. Encourage the user
to speak more. If the user makes a mistake, either correct it subtly in your response or provide a
hint after your reply.”), conversation context (recent dialogue turns), and the user’s latest
utterance (transcribed). The LLM’s completion will be the AI’s reply in Cantonese. We will likely 
truncate or summarize older context if it grows too large, to stay within token limits.
Error Handling & AI Guidance: The prompt will also instruct the LLM to handle cases where the
user’s input was unclear or mispronounced (as indicated by an odd transcription). For example, if
the ASR text is garbled or nonsensical, the LLM/tutor might say in Cantonese, “I’m sorry, could
you repeat that?” or attempt to guess the user’s intent from context. This makes the system
robust to recognition errors or learner mistakes.
Conversational Depth: The AI should ask questions and introduce new sub-topics to keep the
dialogue flowing. If the user seems stuck or doesn’t know what to say, the tutor might ask a
guiding question (in Cantonese) or switch to a new related topic, mimicking a natural
conversation partner who keeps the interaction active.
Pronunciation and Language Feedback: A distinguishing feature is that the AI will provide 
feedback on the user’s Cantonese. This can happen in two ways:
Implicit Feedback: The AI tutor may naturally correct the user by rephrasing. For example, if the
user uses a wrong word or incorrect structure, the tutor’s next reply can incorporate the correct
usage (reinforcing it) without explicitly saying “that was wrong”. This is similar to how human
tutors recast errors.
Explicit Feedback Mode: Additionally, the system offers an option for explicit feedback, either
after each turn or at the end of a session. In this mode, the LLM (or a secondary LLM call)
analyzes the user’s last utterance or overall conversation and generates constructive feedback.
For instance, it might say (in either Cantonese or the user’s native language for clarity): “Your
3
pronunciation of the word 早餐 (breakfast) was a bit unclear – the tone was slightly off. It should
be pronounced with a rising tone. Let’s practice that.” Or: “I understood what you said, but a
native speaker would more likely say it this way: [correct sentence]”. The PRD requires
pronunciation feedback, so we will implement this as a feature the user can toggle or request
(e.g. a “Feedback” button that, when pressed, triggers the AI to give a brief critique of the user’s
last statement).
• 
• 
LLM-based Evaluation: To provide this feedback, we will either use the same LLM with a special
prompt (feeding the user’s utterance and asking for pronunciation and grammar evaluation) or
use a specialized service. Notably, Microsoft’s Speech service offers a Pronunciation
Assessment feature that can score a user’s speech against a reference text
5
. Since our use
case is free-form conversation, a feasible approach is: the LLM can internally generate what it 
expected the user to say (the corrected version of the user’s sentence) and then we utilize Azure’s
pronunciation assessment on the user’s audio vs. that expected sentence to pinpoint
mispronounced words. This hybrid approach combines LLM understanding with a dedicated
pronunciation scoring algorithm. (This is an advanced implementation detail — initially, we may
rely on the LLM’s own analysis for feedback).
Safety and Appropriateness: The LLM will be instructed to avoid inappropriate content. As a
tutor, it should refuse or deflect if the user attempts to steer the conversation to prohibited
areas. This will be handled via the system prompt and, if using OpenAI, their built-in content
f
ilters.
4. Cantonese Speech Synthesis (TTS)
• 
• 
• 
6
High-Quality Cantonese TTS: The system will use a Text-to-Speech service to convert the AI’s
text responses into spoken Cantonese audio for the user to hear. We will integrate an API that
supports Cantonese output. Both Google Cloud Text-to-Speech and Microsoft Azure TTS provide
Cantonese voices. For example, Azure offers neural voices for Cantonese such as HiuMaan
(female voice) and WanLung (male voice), which are Cantonese (Traditional, Hong Kong) voices
. These voices are natural-sounding and suitable for modeling native pronunciation. We will
choose one default voice (e.g. a friendly female voice) for the tutor, with an option to switch if
desired.
Speech Output Behavior: Every time the LLM produces a response, the backend will call the TTS
service with the Cantonese text. The resulting audio (e.g. MP3 or OGG stream) is sent to the
client and played so the user hears it. The frontend will seamlessly play the audio snippet for the
user after each AI turn. There will also be a visual transcript (the text from the LLM) displayed in
a chat bubble, so the user can read what was said (in Chinese characters, and possibly an English
translation or romanization if we choose to display that as support).
Pronunciation Modeling: Using TTS ensures the learner hears correct Cantonese
pronunciation, helping them mimic the accent and intonation. We specifically require a 
Cantonese dialect voice (Yue), not a Mandarin voice, since many Chinese TTS default to
Mandarin. The chosen TTS voice must handle Cantonese tones and colloquial phrases accurately.
According to documentation, Google’s TTS and Amazon Polly have Cantonese support as well
(Polly, for instance, introduced a Cantonese voice “Hiujin” in 2022
7
include naturalness and clarity of the voice for learning purposes.
• 
). Our selection criteria will
Latency and Caching: The TTS conversion will be near real-time (typically responses come in
under a second or two for short sentences). To reduce any noticeable delay, we might pipeline
the calls – for instance, as soon as the LLM text is received, we send to TTS while simultaneously
sending the text to the frontend for display. We may also cache commonly used tutor phrases or
acknowledgments (e.g. “很好！” meaning “Great!”) as audio files, so we can instantly play them if
they occur frequently.
4
• 
Volume and Playback Controls: The frontend will include basic audio controls – ensuring the
TTS audio is played at a comfortable volume. Users can replay the tutor’s last spoken sentence
by clicking on the text (in case they want to hear it again), and possibly adjust the speaking
speed via TTS settings (some TTS APIs allow prosody adjustments).
5. Pronunciation Feedback and Scoring
• 
• 
• 
On-Demand Pronunciation Assessment: The user can request a detailed pronunciation
analysis for something they said. For example, after speaking a sentence, the user might click
“Evaluate Pronunciation”. The system will then either use the LLM or a dedicated pronunciation
API to score the user’s pronunciation. If using Azure’s Pronunciation Assessment, we supply the
user’s audio and the expected text, and it returns scores (overall accuracy, fluency, and word
level scores)
8
9
. This feedback can be presented to the user as a small report: e.g.,
“Pronunciation Score: 85/100. You spoke clearly overall. The words nei5 (你) and ho2 (好) were
pronounced accurately, but sik6 (食) was a bit off in tone.” (Using Jyutping tone numbers in the
feedback for clarity).
Visual Feedback Cues: To complement numeric scores, the UI can highlight the transcribed
sentence with color-coding: words that were pronounced well appear in green, while words with
poor pronunciation appear in red or underlined. Hovering over a problematic word could show a
tooltip with tips (e.g. “Tone should be rising”). This gives immediate, pinpointed feedback for self
correction.
Continuous Improvement Tracking: The system can log pronunciation scores over time. For
example, if a user repeatedly struggled with certain sounds (like the “ng” final or certain tones),
we could detect that pattern. Over the course of sessions, we expect improvement – one of our
success metrics (detailed later) is that users’ pronunciation scores or ASR confidence improve
with practice.
6. User Interface and Experience
• 
• 
• 
• 
Desktop Web UI: A responsive web application interface optimized for desktop screens. It will
have a chat-style layout: one column for the conversation history (alternating user and AI
messages, with text and an icon to play audio for each message), and possibly a sidebar or
header for controls.
Mic and Audio Controls: A prominent microphone button allows the user to start/stop
speaking. When active, it indicates recording (e.g. changing color or icon animation). If voice
activity detection is implemented, the user can also simply speak after the AI finishes, and the
app will auto-detect that they started speaking. The user’s browser will need to grant
microphone access for the site. The audio output is automatic, but volume can be adjusted; also,
a replay button next to each AI message lets the user hear it again if needed.
Bilingual Support (for clarity): The interface language (menus, instructions) can be in Chinese
(Simplified or Traditional based on user preference) since the target users are Chinese speakers.
The conversation content remains in Cantonese, displayed in Traditional Chinese characters
(since Cantonese learners typically use Traditional characters in Hong Kong context). Optionally,
we might include a toggle for subtitles: for each Cantonese AI message, the user could reveal a
Mandarin translation or phonetic transcription (Jyutping) if they didn’t understand the
Cantonese. This is a support feature to aid comprehension, but it’s user-controlled to keep the
experience immersive by default.
Session Management: Users can end a conversation and start a new one at any time (which
resets the context). We will also implement login and progress saving (e.g. through a simple
account system) so that users can keep track of how many conversations they’ve had. Logged-in
users might have access to a history of their past conversation transcripts for review. However,
5
for initial launch, an account may not be mandatory – a quick start “Practice Now” should be
available.
• 
Error Alerts: If something goes wrong (e.g. ASR fails or network issues), the UI will display a
polite error message and possibly prompt the user to retry. For instance, “Sorry, I didn’t catch
that. Please check your microphone or try again.” We will design these to be minimal
interruptions.
Recommended AI Technologies and Services
This application leverages three main AI components: ASR, LLM, and TTS. Below we recommend
optimal APIs/services for each, considering Cantonese support and production reliability:
• 
• 
• 
3
2
Automatic Speech Recognition (ASR): We recommend using a cloud-based speech-to-text API
with proven Cantonese support. Google Cloud Speech-to-Text is a top choice, as it natively
supports Cantonese (Traditional Hong Kong dialect) with language code yue-Hant-HK . It
offers high accuracy and features like automatic punctuation. Microsoft Azure Cognitive
Services Speech is an equally strong option; it supports Cantonese (zh-HK for Hong Kong
Cantonese) in real-time transcription and even has ancillary features like profanity filtering and
custom vocabulary . Amazon Transcribe is another viable service, supporting Cantonese in
streaming mode , though its accuracy for Cantonese has been less documented publicly
compared to Google/Azure. We will likely start with Google’s API for ASR, given positive accuracy
reports, and potentially allow switching to Azure’s if needed (the architecture abstracts the ASR
provider). If an on-premise or offline solution is required in the future, we could integrate
OpenAI’s Whisper model (which has a Cantonese model available ), but this would involve
running a heavy model on our servers – a trade-off for avoiding external APIs. Initially, cloud ASR
is preferred for ease and accuracy.
4
1
10
Conversational LLM: The AI tutor’s brain will be OpenAI’s GPT-4 (accessible via the OpenAI API
or Azure OpenAI Service). GPT-4 is state-of-the-art and has demonstrated strong capabilities in
understanding and generating Chinese text, including Cantonese. ChatGPT/GPT-4 officially
supports a wide range of languages (over 50), including Cantonese (often referred to as Yue
Chinese) . This ensures the model can comprehend the user’s Cantonese input (which will be
in Chinese text form post-ASR) and respond with grammatically correct, contextually appropriate
Cantonese. In case GPT-4 is too costly or if slightly lower-quality responses are acceptable for
higher volume, OpenAI’s GPT-3.5 Turbo can be used as a fallback – it also supports Cantonese,
albeit with possibly less nuanced output. We must note that GPT-4 (as of 2025) is an online
service with usage costs and rate limits; we will design around these, caching certain responses
if possible and handling rate limit errors gracefully. As an alternative for future consideration
(especially if data privacy or self-hosting becomes a concern), we could evaluate local Chinese
LLMs such as Baidu ERNIE or open-source models fine-tuned on Chinese. However, those may
not be optimized for Cantonese colloquialisms. For launch, GPT-4 via API provides the fastest
route to a high-quality conversational experience.
Text-to-Speech (TTS): For voice output, we recommend Microsoft Azure’s Neural Text-to
Speech service with a Cantonese voice. Azure offers multiple Cantonese (zh-HK) voices like
HiuMaanNeural (female), HiuGaaiNeural (female), and WanLungNeural (male) . These neural
voices are highly natural and specifically designed for Cantonese, with proper tone and
inflection. We can pick one as the default voice of the AI tutor (for instance, “HiuMaan” which is a
friendly female voice) – this will represent the AI’s personality. Azure’s TTS has the advantage of
quick response generation and high quality. Alternatively, Google Cloud Text-to-Speech also
has Cantonese voices (Standard voices as listed under yue-HK) , and Amazon Polly recently
6
11
6
7
introduced a Cantonese voice (“Hiujin”) . We will evaluate the quality of each; early tests
indicate Azure’s voices sound very clear and are likely the best choice for a professional tutoring
experience. Additionally, Azure allows fine control (SSML) for pronunciation if needed (we could
tweak how certain words are read, though this might rarely be needed if the text is in Chinese
characters). In summary, our stack might use Google for ASR and Azure for TTS to leverage the
best of each platform. This is feasible since the front-end/backend can call different APIs
independently. Of course, to simplify vendor management, we could also do all on Azure (Azure’s
ASR and TTS together) – Azure’s ASR quality for Cantonese is known to be strong , so this is an
acceptable alternative if we want to consolidate services. The final decision will weigh factors like
cost per hour of audio, latency, and licensing.
3
Each of these services (Google, Azure, OpenAI) require API credentials and have usage costs. The
system will be designed to easily swap out one service for another via configuration, in case we need to
change providers or support deployment in different regions (for example, a deployment for mainland
China might use a local ASR/TTS provider due to firewall issues). All network calls to these APIs will be
made from the backend server to keep keys secure and to funnel data through a single point for
logging.
System Architecture and Component Design
The application follows a client–server architecture with integration of external AI services (ASR, LLM,
TTS). Below we describe each component and their interactions.
Figure: High-level architecture and data flow. The system comprises a web front-end (running in the
user’s browser) and a backend server that orchestrates AI service calls. The numbered steps
correspond to a single user utterance and AI response cycle, detailed as follows:
1. 
2. 
User Interface (Browser Frontend): The user interface is a Single Page Application (SPA) (e.g.
built with React or Vue) that manages audio recording and playback, and displays the chat
transcript. When the user is ready to speak, they click the microphone button. The browser uses
the Web Audio API or MediaRecorder to capture the user’s voice input. The raw audio stream
(step 1 in the figure) is sent from the frontend to the backend over a secure connection (we may
use a WebSocket for streaming audio or HTTP POST for short utterances).
Backend Server: The backend (could be a Node.js Express app or Python Flask/FastAPI app)
receives the audio data (step 2). It temporarily buffers the audio (possibly in memory or on disk if
7
needed) and then forwards it to the ASR service for transcription. This could involve calling
Google’s Speech-to-Text API with the audio bytes and language code 
yue-HK . The backend
might call a streaming endpoint if available, or send the whole audio and wait for the
transcription result.
3. 
4. 
5. 
6. 
7. 
8. 
Speech-to-Text Processing: The ASR service processes the audio and returns a text transcription
of what the user said (step 3). For example, the user’s speech “我想學講廣東話” might be
transcribed to exactly those Chinese characters if recognized correctly (“I want to learn to speak
Cantonese”). The backend receives this text along with possibly metadata (confidence scores,
etc.). If confidence is low or the transcript seems to have errors (we can do a basic check), the
backend might decide to handle that (for now, we will assume the transcription is usable).
Dialogue Management & LLM Request: The backend now formulates a request to the LLM. It
will assemble the prompt with the necessary context (system instructions + recent conversation
+ user’s new utterance). This prompt is sent to the LLM API (step 4). We’ll likely use OpenAI’s
RESTful API endpoint (e.g. 
v1/chat/completions for ChatGPT models) to get a completion.
The request includes parameters like model name (e.g. 
gpt-4 ), temperature (we might use a
moderate temperature ~0.7 for varied but relevant responses), and the prompt content. The
network call goes out to the LLM provider’s server.
AI Response Generation: The LLM processes the input and generates a response (in Cantonese)
along with any feedback notes if prompted. The response is returned to our backend (step 5).
For example, the AI might return a message: “真好！你點解想學廣東話?” (“That’s great! Why do
you want to learn Cantonese?”). The backend parses the LLM’s output. If we requested the LLM
to also produce a separate evaluation of the user (possibly hidden or in a structured format), we
would extract that here too. For instance, the LLM might have replied with a special token or
JSON containing feedback like 
{pronunciation: "good", mistake: null} which we can
use to formulate a feedback message. This is an implementation detail to be refined; initially, the
LLM will likely just return the tutor’s speech.
Text-to-Speech Synthesis: The backend now takes the AI’s response text and calls the TTS
service to synthesize speech (step 6). For Azure, this means making an HTTP request to the TTS
endpoint with parameters like voice name (e.g. 
zh-HK-HiuMaanNeural ), output format, and
the text. The TTS service returns an audio file/stream (e.g. WAV or MP3 data) of the Cantonese
speech (step 7). The backend receives this audio data. We will ensure the TTS API is called in a
way that the response is quick – since the text is usually one sentence or two, the TTS generation
should be fast (a few hundred milliseconds to a second).
Frontend Playback and Display: The backend sends the AI’s text and the audio back to the
frontend (step 7). If using HTTP, this can be a JSON response containing the text and a URL or
binary data for audio. If using WebSocket, the server can push an event containing the same.
The frontend then does two things in parallel: it renders the AI’s message in the chat UI
(appending it as a new message bubble with the text), and it plays the audio output so the user
hears the Cantonese sentence spoken (step 8, playing through the browser’s audio output). The
UI may animate or highlight the message currently being spoken (karaoke-style highlighting is a
nice-to-have for indicating which words are being spoken).
Feedback to User: If feedback on the user’s pronunciation or grammar is to be given every turn
(depending on user settings), the backend/LLM would have included that in the response or as a
separate payload. The frontend can then show this feedback. For instance, it might show a small
8
italicized note under the user’s own bubble like “(Pronunciation note: try a rising tone on the second
word next time)”. Alternatively, the tutor AI can speak the feedback in Cantonese too (“你啱啱嗰句
說得幾好，不過聲調可以再清楚啲。”). We will decide via user testing whether immediate explicit
feedback is helpful or if it’s better on demand to not disrupt conversational flow. Likely, by
default the AI’s spoken response will focus on conversation, and any corrections will be subtle.
The user can tap a “Feedback” button to get a more detailed comment from the AI.
9. 
10. 
Conversation Continuation: The cycle repeats: the user speaks a response to the AI’s question,
and the loop of ASR -> LLM -> TTS runs again. This continues until the user ends the session.
Session Wrap-up: When a session ends, the system could compile a brief session summary. For
example, the LLM could generate a summary of new words that came up or tips for
improvement, which could be shown to the user (“Today you talked about hobbies. You learned 3
new words: … Also, remember to pronounce the third tone in ….”). This summary can be stored
in the user’s history for review.
Technical Stack Considerations: The front-end will be implemented with web technologies (HTML5, JS,
and a framework like React). It will utilize Web APIs for audio recording and playback. The backend will
be a web service running on the cloud (for example, an AWS EC2 or Azure App Service instance). It will
expose endpoints for the front-end to interact (e.g. 
POST /api/recognize_and_respond to handle
the main cycle). We will use secure protocols (HTTPS/WSS) to transmit audio and data, as user speech
might be sensitive. The external AI service credentials (API keys for Google, OpenAI, Azure) are stored
securely on the server and never exposed to the client.
Data Storage: While most interactions are real-time, some data will be stored: - User profiles (if login is
supported) in a database. - Conversation logs (text transcripts, possibly audio recordings if user
consents for analysis). These can help in improving the AI or reviewing user progress. We will include a
privacy policy and obtain consent if we store voice data. - Caches for API results (maybe caching TTS
audio for repeated AI phrases, to save costs and time).
Scaling: The architecture allows scaling each component horizontally. The stateless backend can be
replicated behind a load balancer; it relies on external managed services for the heavy AI tasks. We will
monitor throughput (ASR and TTS can handle concurrent requests as per their quotas, and the LLM calls
will be rate-limited by OpenAI API limits, which we need to manage by perhaps queuing or restricting
simultaneous sessions if needed). Real-time performance is crucial, so we will optimize the pipeline to
have minimal idle times between ASR result and LLM call, etc. If usage grows, we might consider
batching some operations (though conversation is interactive, so batching is limited).
Data Flow and Component Interaction
The component interaction has been outlined above with the numbered steps. To reiterate in a
sequence form:
• 
• 
• 
• 
• 
• 
Front-end: Initiates audio capture -> sends audio to backend.
Backend: receives audio -> calls ASR API -> gets text.
Backend: calls LLM with text and context -> gets reply text (and possibly feedback).
Backend: calls TTS with reply text -> gets audio.
Backend -> Front-end: returns reply text + audio to client.
Front-end: plays audio, shows text -> loop continues.
9
All these interactions happen within a few seconds, aiming for a smooth conversational cadence. The
user essentially experiences it as talking to a responsive tutor: they speak, a short pause happens
(during which ASR/LLM/TTS work in the background), and then they hear the tutor’s reply.
We have designed the system such that each specialized task is handled by the service best suited for it
(speech recognition by ASR engine, language understanding by LLM, speech output by TTS). This
modular approach means we can upgrade or swap any component without major changes to others
(for example, if a new more accurate Cantonese ASR becomes available, we can integrate that in place
of the current one).
Performance Metrics and Success Criteria
To ensure the product delivers value and functions reliably, we define the following performance
metrics and success criteria:
1. Speech Recognition Accuracy: The ASR component should achieve a high accuracy for
conversational Cantonese. We will measure Word Error Rate (WER) on sample user inputs. Success is
defined as a WER of < 10% for clear input in supported scenarios (e.g., common phrases) and a
reasonable WER (under 20%) for more complex or accented input. Additionally, ASR confidence scores
per utterance should generally be high (we expect average confidence > 0.9 on a 0-1 scale for correctly
recognized phrases). We will gather data during beta testing to see how often the AI had trouble
understanding the user due to ASR errors. A target metric: 95% of user utterances are correctly
understood (i.e., do not require the AI to ask for repetition).
2. Response Relevance and Coherence: The quality of the AI tutor’s responses (from the LLM) will be
measured by user feedback and conversational evaluations. We want the AI to stay on topic, provide
helpful answers, and not produce incorrect or confusing statements. A qualitative success criterion is
that users rate the AI responses at least 4 out of 5 in relevance and helpfulness in post-session
surveys. We can also use conversation transcripts to perform an expert evaluation: the tutor should not
produce any critical errors (like responding in the wrong language, or giving blatantly wrong
information about Cantonese). Ideally, in > 90% of turns, the AI’s utterance is judged “appropriate and
helpful” by evaluators.
3. Pronunciation Feedback Efficacy: For the feedback feature, we will measure how often users utilize
it and whether it leads to improvement. A metric could be pronunciation score improvement over
time: for example, if a user’s average pronunciation rating (by the system) in week 1 is 70/100, after a
month of use it should increase (say to 80/100). User success criteria: at least 80% of active users show
measurable improvement in pronunciation or speaking confidence after 1 month (as per self-reported
feedback or pre/post testing). Another metric: the system’s pronunciation assessment accuracy – if
compared to a human teacher’s evaluation, the system’s feedback should agree in the majority of cases.
We can pilot this with a small user group and have a native speaker review the system’s feedback for
correctness.
4. Latency (Responsiveness): The time from when a user finishes speaking to when the AI voice begins
responding should be short enough to feel conversational. Our goal is to keep this turnaround time
under 3 seconds on average. Breaking it down: ASR ~1s, LLM ~1s, TTS ~0.5s, plus network overhead.
We will log and monitor latency. Success is if 95th percentile latency is <5 seconds and average <3
seconds. If we find latency creeping up (maybe due to LLM load), we will consider mitigation like
streaming partial responses (e.g. start TTS speaking while LLM is still generating – though that’s
complex). A related metric is system uptime and reliability: The service should be available >99% of
10
the time, and conversation rounds should complete without errors in >98% of turns. We’ll use
monitoring to track any failed API calls or timeouts and address them.
5. User Engagement and Satisfaction: As a product, a key success indicator is that users find it useful
and engaging. Metrics: - Session Length & Frequency: We expect users to engage in ~5-10 minute
conversations. If average session length is consistently under 2 minutes, it might indicate users lose
interest or get frustrated quickly. We aim for an average session length of at least 5 minutes, and
power users doing multiple sessions. Also, return rate: at least 50% of users who try it should come
back for another session, indicating they found value. - User Satisfaction/NPS: Through in-app surveys
or feedback forms, we’d like to see a majority of positive feedback. For instance, a target is an NPS (Net
Promoter Score) of >30 or simply >80% of users rating the experience “good” or “excellent”. 
Qualitative Success: Testimonials or feedback from users saying things like “this app helped me
practice speaking Cantonese and I feel more confident now” is a strong sign we are meeting the core
need. We will collect such feedback during beta testing.
6. Learning Efficacy: Ultimately, the product’s goal is to improve the learner’s Cantonese. While harder
to measure in the short term, we can simulate or test improvement. One idea is to give users a short
oral test at the beginning and after some usage period. Success would be demonstrable improvement
in that test (for example, the user can now carry out a role-play dialogue more fluently or with fewer
errors than before). If working with language instructors or institutions, we might align with CEFR
speaking levels or similar proficiency metrics to evaluate progress.
7. Technical Success Criteria: On the engineering side, success means the system is maintainable,
scalable, and secure: - The system should handle at least 100 concurrent users in real-time
conversation without significant degradation (this is a scalability benchmark for our initial target; we
can scale further if needed by adding servers). - Memory and CPU usage on the backend should remain
within limits (no memory leaks; each audio/LLM cycle should not bloat resources). - The integration of
external APIs should be resilient – e.g., if the ASR API fails mid-session, the system catches the error and
prompts the user to retry, rather than crashing. - From a security/privacy standpoint, all user data
(audio and text) is transmitted securely, and personal data is handled in compliance with relevant laws
(since our target is likely in Hong Kong/Asia, we’ll consider compliance with PDPO or GDPR if applicable
for international users).
We will treat these metrics as our Success Criteria. Pre-launch, we will do internal testing to ensure
these thresholds can be met. Post-launch, we will monitor actual usage data to verify if adjustments are
needed (e.g., if ASR accuracy is below target for certain speaker accents, we might train a custom model
or switch provider).
To summarize, this Product Requirements Document has outlined a comprehensive plan for a
Cantonese conversational tutor application. We defined the scope centered on spoken dialogue
practice, detailed how users will interact with the AI tutor, listed the features (from ASR to feedback)
needed to fulfill that experience, recommended specific AI services (with Cantonese support) to
implement each feature, and described the technical architecture that ties everything together. By
measuring the performance and success criteria above, the development team can ensure the product
is not only functionally complete but also effective in helping users learn Cantonese in a natural,
engaging way. The next steps would include designing the UI flow in detail, setting up the API accounts/
infrastructure, and building a prototype to iterate on these requirements with real user feedback in
mind. With this PRD, the development team should have a clear blueprint to proceed without further
clarification. 
11
1
Cloud Speech-to-Text V2 supported languages  |  Google Cloud Documentation
https://docs.cloud.google.com/speech-to-text/docs/speech-to-text-supported-languages
2
Supported languages and language-specific features - Amazon Transcribe
https://docs.aws.amazon.com/transcribe/latest/dg/supported-languages.html
3
5
Language support - Speech service - Foundry Tools | Microsoft Learn
https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support
4
Languages Supported by ChatGPT
https://explodingtopics.com/blog/chatgpt-supported-languages
6
Azure AI Voices in Chinese
https://json2video.com/ai-voices/azure/languages/chinese/
7
Available voices - Amazon Polly - AWS Documentation
https://docs.aws.amazon.com/polly/latest/dg/available-voices.html
8
9
Does Azure Pronunciation Assessment handle Hong Kong, Japanese, and other East Asian
English accents accurately? - Microsoft Q&A
https://learn.microsoft.com/en-us/answers/questions/2337244/does-azure-pronunciation-assessment-handle-hong-ko
10
Transcribe Cantonese Speech to Text: With Code Samples and Automated Batch Processing
Techniques
https://library.hkust.edu.hk/sc/cantonese-speech-to-text/
11
Supported voices and languages  |  Cloud Text-to-Speech  |  Google Cloud Documentation
https://docs.cloud.google.com/text-to-speech/docs/list-voices-and-types
12  transfer all of these into a markdown file don't miss any words
